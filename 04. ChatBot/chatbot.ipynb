{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fd9d67-7d3f-4f8a-9216-83e94fbe6949",
   "metadata": {},
   "source": [
    "<h1>ChatBot</h1>\n",
    "\n",
    "In this notebook, we will try to implement a chatbot that responds to human language. This chatbot has a set of predefined answers and chooses the most proper one depending on the message it receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7aec34-26ff-4e14-850a-9e5ff429579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ab8c9-8f90-4fa7-9570-e91e62a26817",
   "metadata": {},
   "source": [
    "Reading the intents file and creating $(X, y)$ where $X$ represents the sentences and $y$ represents the target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2437d5-2a8a-4844-a0bc-32e1db5ae567",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "    \n",
    "\n",
    "classes = []\n",
    "patterns = []\n",
    "target = []\n",
    "\n",
    "for i in range(len(intents['intents'])):\n",
    "    classes.append(intents['intents'][i]['tag'])\n",
    "    for j in range(len(intents['intents'][i]['patterns'])):\n",
    "        patterns.append(intents['intents'][i]['patterns'][j])\n",
    "        target.append(intents['intents'][i]['tag'])\n",
    "\n",
    "classes = np.array(classes)\n",
    "target = np.array(target)\n",
    "patterns = np.array(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ccb9982-a4b4-4d9e-b77a-e44fafe04ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary that maps encoded classes to their name\n",
    "\n",
    "target2int = {}\n",
    "\n",
    "for index, category in enumerate(classes):\n",
    "    target2int[category] = index\n",
    "        \n",
    "y = np.vectorize(target2int.get)(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ce8a9-4bec-463e-a547-04593296f953",
   "metadata": {},
   "source": [
    "Preprocessing the sentences. This includes:\n",
    "<ul>\n",
    "    <li>Removing punctuation</li>\n",
    "    <li>Lowering each character so we will deal with fewer words</li>\n",
    "    <li>Tokenizing</li>\n",
    "    <li>Stemming</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cac98c2-6cc6-49f9-9f46-52370be59d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(patterns):\n",
    "    \n",
    "    processed_patterns = []\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        \n",
    "        punct_pattern = (\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\")\n",
    "        punct = re.compile(punct_pattern)\n",
    "        pattern = re.sub(punct, \"\", pattern)\n",
    "        \n",
    "        pattern = pattern.lower()\n",
    "        \n",
    "        pattern = word_tokenize(pattern)\n",
    "        \n",
    "        ps = PorterStemmer()\n",
    "\n",
    "        pattern = [ps.stem(pattern) for pattern in pattern]\n",
    "        \n",
    "        pattern = ' '.join(pattern)\n",
    "                \n",
    "        processed_patterns.append(pattern)\n",
    "\n",
    "    return(processed_patterns)\n",
    "\n",
    "processed_patterns = preprocess(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55dde8-b843-4354-9cff-1ca86cea0cbc",
   "metadata": {},
   "source": [
    "Getting a matrix with $TF-IDF$ entries. This matrix will be used as training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3085c5bf-3b95-4d94-abe5-9c3a6d604199",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0)   \n",
    "X = vectorizer.fit_transform(processed_patterns).toarray()\n",
    "\n",
    "feature_size = X.shape[1]\n",
    "\n",
    "train_data = np.array(list(zip(X, y)), dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04541e5-85f3-4077-aaf9-ac7a2d57f4f2",
   "metadata": {},
   "source": [
    "Getting a data loader for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "985a37e8-8510-44ea-a962-79b727f21f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(batch):\n",
    "    \n",
    "    X, y = list(zip(*batch))\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "        \n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b769e850-0d88-498a-b536-9f7b9c32f8e4",
   "metadata": {},
   "source": [
    "Simple feed forward neural network to classify each user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86bcdcee-6477-472a-8862-1636223fd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeedForwardClassifier, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(feature_size, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128,16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(16, len(classes)),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1de37-9cf8-464f-b015-65d0947bf3e7",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa61b3f-c8af-49bc-9e67-ffef6ea965fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = FeedForwardClassifier()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=0.00)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a2418-ce79-4ece-837a-d8272e3f1a4e",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e79e93-9e48-489e-a2c6-bb50b43cee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, Train loss = 1.856 \n",
      "\n",
      "epoch 10, Train loss = 1.414 \n",
      "\n",
      "epoch 15, Train loss = 1.048 \n",
      "\n",
      "epoch 20, Train loss = 0.813 \n",
      "\n",
      "epoch 25, Train loss = 0.681 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epochs):\n",
    "     \n",
    "    losses = []\n",
    "    \n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "                \n",
    "        probs = model(X)\n",
    "        \n",
    "        loss = loss_fn(probs, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "        \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"epoch {i + 1}, Train loss = {np.array(losses).mean():.3f} \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "600bbf6a-fafd-44dd-8abf-45340e14aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a336e-dea3-45a4-8943-96d1d3882227",
   "metadata": {},
   "source": [
    "Handle new user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f4a283c-c4d2-43ea-a728-c05535656430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccess_new_input(prompt):\n",
    "    prompt = preprocess(prompt)\n",
    "    prompt = vectorizer.transform(prompt).toarray()\n",
    "    prompt = torch.from_numpy(prompt).to(torch.float32)\n",
    "    return(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1832c4c3-a753-40e1-b505-8644770bdac4",
   "metadata": {},
   "source": [
    "Loop for interacting with the bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e3a981-d574-4b6c-ae1d-9237b5348b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi. How can i help you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " WHat do you have?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the newest novels.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " How can i pay?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We take paypal, venmo and cash!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " WHat would you recommend?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hitchhiker's Guide To The Galaxy by Douglas Adams\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Thanks bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have a nice day!\n"
     ]
    }
   ],
   "source": [
    "intents = intents['intents']\n",
    "\n",
    "\n",
    "while (True):\n",
    "    \n",
    "    usr_input = [input()]\n",
    "    \n",
    "    processed_input = proccess_new_input(usr_input)\n",
    "    \n",
    "    probs = model(processed_input)\n",
    "    \n",
    "    predict = probs.argmax()\n",
    "       \n",
    "    responses = intents[predict]['responses']\n",
    "    \n",
    "    response = random.sample(responses, 1)[0]\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    if (intents[predict]['tag'] == \"goodbye\"):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299b826d-7f51-4f5e-8133-483d14466746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee7461-54a5-4901-b34d-ba300a81347f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
