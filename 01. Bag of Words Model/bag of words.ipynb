{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f438d9cc-7169-4769-a760-af8cc78fb665",
   "metadata": {},
   "source": [
    "<h1><center> Sentiment Analysis of Movie Reviews using Bag-of-Words (BoW) model </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e15f0-72e9-4123-8839-c9ed3a6b73ab",
   "metadata": {},
   "source": [
    "The bag-of-words model is a primitive NLP model. This model is simple to understand and implement and has seen great success in problems such as language modeling and document classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceac1d2-ba82-4806-a11b-9a257d1583f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17899e44-dc89-43b0-8ece-073ce0cc1279",
   "metadata": {},
   "source": [
    "Let's first read the data. For this implementation, i will use <a href=\"https://www.kaggle.com/marklvl/sentiment-labelled-sentences-data-set\"><b>Sentiment Labelled Sentences Data Set<b></a> on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b581f2c2-0664-4305-b21f-e289b4184b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c2be35-7b02-4786-b3bd-e38d2febf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"./sentiment labelled sentences/amazon_cells_labelled.txt\", delimiter=\"\\t\", header=None)\n",
    "data2 = pd.read_csv(\"./sentiment labelled sentences/yelp_labelled.txt\", delimiter=\"\\t\", header=None)\n",
    "data3 = pd.read_csv(\"./sentiment labelled sentences/imdb_labelled.txt\", delimiter=\"\\t\", header=None)\n",
    "\n",
    "reviews_df = pd.concat([data1, data2, data3])\n",
    "reviews_df.columns = [\"Review_text\", \"Review_class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17dc6fc5-4d30-4cb5-9641-ebeade2b8be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_text</th>\n",
       "      <th>Review_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review_text  Review_class\n",
       "0  So there is no way for me to plug it in here i...             0\n",
       "1                        Good case, Excellent value.             1\n",
       "2                             Great for the jawbone.             1\n",
       "3  Tied to charger for conversations lasting more...             0\n",
       "4                                  The mic is great.             1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c4e11-c15e-40f5-81ba-16bdd123fd3f",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62005c10-bd67-4b39-981a-1be7bdf12a0a",
   "metadata": {},
   "source": [
    "Cleaning data is an important part of any NLP model. Here i will use <emph>NLTK</emph> (Natural Language Toolkit) for cleaning data. This process consists of $n$ steps:\n",
    "<ol>\n",
    "  <li> Converting all characters to lower case </li>\n",
    "  <li> Removing any possible link </li>\n",
    "  <li> Removing punctuation</li>\n",
    "  <li> Stemming, i.e process of reducing inflected words to their root form </li>\n",
    "  <li> Removing <emph>Stopwords</emph>. Stop words are a set of commonly used words in a language. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3989a9cb-a2a3-47b8-a7b3-951b9aee2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c9bd32d-414f-4371-ba11-dcb84ec7bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(document):\n",
    "    \n",
    "        # everything should be lower case\n",
    "        document = document.lower()\n",
    "                \n",
    "        # remove any possible link\n",
    "        links_pattern = \"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\"\n",
    "        links = re.compile(links_pattern)        \n",
    "        document = re.sub(links, \"\", document)\n",
    "        \n",
    "        # remove punctuation\n",
    "        punct_pattern = (\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\")\n",
    "        punct = re.compile(punct_pattern)\n",
    "        document = re.sub(punct, \"\", document)\n",
    "        \n",
    "        # stemmer\n",
    "        ps = PorterStemmer()\n",
    "        words = word_tokenize(document)\n",
    "        \n",
    "        # a list of stop words\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\") # i had an intuition that not shouldn't be removed\n",
    "        \n",
    "        # process of stemming and removing stopwords\n",
    "        words = [ps.stem(word) for word in words if not (word in stop_words)]\n",
    "        \n",
    "        words = ' '.join(words)\n",
    "        \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b4e2c42-f111-4f52-998c-3428ba2c6b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(dataframe):\n",
    "    clean = list()\n",
    "    reviews = dataframe[\"Review_text\"].values.tolist()\n",
    "    for document in reviews:\n",
    "        \n",
    "        words = clean_string(document)\n",
    "        \n",
    "        clean.append(words)\n",
    "       \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d0376a-dd6c-44b4-9b4d-a2b7894f8fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['way plug us unless go convert',\n",
       " 'good case excel valu',\n",
       " 'great jawbon',\n",
       " 'tie charger convers last 45 problem',\n",
       " 'mic great']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews = clean_data(reviews_df)\n",
    "clean_reviews[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190acc4-3cbb-432b-a876-f16f6bf64cca",
   "metadata": {},
   "source": [
    "## Making the Feature Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ceee92-5711-4d4e-8c27-61fc1e108371",
   "metadata": {},
   "source": [
    "The next step is to turn each document of free text into a vector that we can use as input for a machine learning model. In order to do so, first we obtain a <b>dictionary</b> of unique words in corpus. Then we can create a matrix where each row corresponds to a document in corpus and each column corresponds to a word in the dictionary of unique words. The $<i, j>$ element of feature matrix is set to 1 if the $ith$ document contains the $jth$ word and 0 otherwise.\n",
    "<br>\n",
    "To do this, we will use the CountVectorizer from <i>sklearn</i>.\n",
    "<br>\n",
    "As it can be seen, we end up with a dataset with 2748 rows and 1208 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e11cc6d7-d59d-4961-b3d8-5ed7c545770b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2748, 1208)\n",
      "(2748,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=3)   \n",
    "X = vectorizer.fit_transform(clean_reviews).toarray()\n",
    "y = reviews_df.values[:,1].astype('int')\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56936195-4ded-41af-bce3-131e7d241ed5",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dffd1a-3b73-48ef-8421-f97048998901",
   "metadata": {},
   "source": [
    "Split the feature matrix into random train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2919a9a8-c0c7-4883-89f9-8708c1f8e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c718e-980e-4116-b1a0-de3d5a7f05c4",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2985803-a066-43ef-bd1b-aa7600e2f940",
   "metadata": {},
   "source": [
    "After we obtain a dataset, we can use any machine learning algorithm to fit a model to it. Here will will use <i>Random Forest</i> amd <i>Naive Bayes</i> seperately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd291c2-3b1f-4ec2-a907-372fcd6bbd2d",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce44816-cdea-4f18-a333-03519ceeba63",
   "metadata": {},
   "source": [
    "Fitting a random forest model to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ec1fdd-8c40-4d6b-b18e-16599264588b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_model = RandomForestClassifier()\n",
    "\n",
    "RF_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49466404-6b6d-4a75-aa23-369a376d4f0f",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa7a19aa-8ab7-429e-89ad-9c0de1fb5cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 81.818%\n",
      "f1 score: 0.819\n",
      "precision score: 0.825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "\n",
    "\n",
    "y_pred = RF_model.predict(X_test)\n",
    "\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred) * 100:.3f}%\")\n",
    "print(f\"f1 score: {f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"precision score: {precision_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f794beef-2585-4f2d-bafa-b53771fa9288",
   "metadata": {},
   "source": [
    "### Giving Manual Inputs to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c65d0cd-bd2d-441b-bf23-321f1f49373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Manual input:  I hated it, it was a waste of time!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the review is a negative review\n"
     ]
    }
   ],
   "source": [
    "new_review = input(\"Manual input: \")\n",
    "clean_x = clean_string(new_review)\n",
    "clean_x = [clean_x]\n",
    "new_input = vectorizer.transform(clean_x).toarray()\n",
    "\n",
    "predic = RF_model.predict(new_input)\n",
    "\n",
    "print(\"the review is a \", end = \"\")\n",
    "\n",
    "if predic:\n",
    "    print(\"positive review\")\n",
    "    \n",
    "else:\n",
    "    print(\"negative review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a710c7-39a0-4771-a6da-ebec29931c82",
   "metadata": {},
   "source": [
    "+---------------------------------------------------------------------------------------------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abbac3-7884-4308-b4f7-adec2efaafd8",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b1cad11-9ba3-410b-8f8f-0baa0d033567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB_model = GaussianNB()\n",
    "\n",
    "NB_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a0134-4d3e-481c-a0c0-ba6f1bfbe384",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "107c6986-a6ee-4692-8f15-08a7619d3fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 68.545%\n",
      "f1 score: 0.725\n",
      "precision score: 0.650\n"
     ]
    }
   ],
   "source": [
    "y_pred = NB_model.predict(X_test)\n",
    "\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred) * 100:.3f}%\")\n",
    "print(f\"f1 score: {f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"precision score: {precision_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc09353-dda3-4692-932f-2ddb44ddf2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Manual input:  I really liked it. Great work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the review is a positive review\n"
     ]
    }
   ],
   "source": [
    "new_review = input(\"Manual input: \")\n",
    "clean_x = clean_string(new_review)\n",
    "clean_x = [clean_x]\n",
    "new_input = vectorizer.transform(clean_x).toarray()\n",
    "\n",
    "predic = NB_model.predict(new_input)\n",
    "\n",
    "print(\"the review is a \", end = \"\")\n",
    "\n",
    "if predic:\n",
    "    print(\"positive review\")\n",
    "    \n",
    "else:\n",
    "    print(\"negative review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7902c2-d8b7-4c4d-b47d-c43729e0a3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
